import pandas as pd
from tqdm.auto import tqdm
import nltk
from nltk.corpus import stopwords
import string
from nltk import word_tokenize
from nltk.stem.snowball import SnowballStemmer
import re

def uploadind_data():
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ CSV –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤
    raitings_df = pd.read_csv('raitings_df.csv')
    people = pd.read_csv('people.csv')
    brons = pd.read_csv('brons.csv')

    # —É–¥–∞–ª–∏–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    raitings_df = raitings_df[~raitings_df['rubric'].str.contains(
        '–ü–∞—Ä–∏–∫–º–∞—Ö–µ—Ä—Å–∫–∞—è|–ë–∞—Ä–±–µ—Ä—à–æ–ø|–ú–µ–±–µ–ª—å –¥–ª—è –∫—É—Ö–Ω–∏|–®–∫–∞—Ñ—ã-–∫—É–ø–µ|–ú–∞–≥–∞–∑–∏–Ω –º–µ–±–µ–ª–∏|–¢–∞—Ç—É-—Å–∞–ª–æ–Ω|–°–ø–æ—Ä—Ç–∏–≤–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ|–§–∏—Ç–æ–ø—Ä–æ–¥—É–∫—Ü–∏—è, –ë–ê–î—ã|–°–ø–æ—Ä—Ç–∏–≤–Ω–∞—è –æ–¥–µ–∂–¥–∞ –∏ –æ–±—É–≤—å|–Æ–≤–µ–ª–∏—Ä–Ω—ã–π –º–∞–≥–∞–∑–∏–Ω|–õ–æ–º–±–∞—Ä–¥|–ú–∞–≥–∞–∑–∏–Ω —á–∞—Å–æ–≤')]
    return raitings_df, people, brons

def data():
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ CSV –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤
    top = pd.read_excel('top_10000.xlsx')
    top_m = pd.read_excel('top_m.xlsx')
    restoran_norm = pd.read_excel('restoran_norm.xlsx')
    tmp = pd.read_excel('tmp.xlsx')
    return top, top_m , restoran_norm, tmp

def clean_text(text):
    stop_words = set(stopwords.words('russian'))
    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = text.translate(str.maketrans('', '', string.punctuation))

    # –£–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä
    text = re.sub(r'\d+', '', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–º–∞–π–ª–æ–≤
    text = re.sub(r':\)|:\(|:-\)', '', text)
    text = re.sub(r'üòä', '', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ –º–∞—Ç–∞
    text = re.sub(r'\b–º–∞—Ç\b', '', text, flags=re.IGNORECASE)

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
    tokens = nltk.word_tokenize(text.lower())
    cleaned_tokens = [token for token in tokens if token not in stop_words]

    return ' '.join(cleaned_tokens)

def clear_data(raitings_df):
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ id –∏ rubric, –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤
    grouped_data = raitings_df.groupby(['id', 'rubric'])['otziv'].apply(lambda x: ' '.join(x)).reset_index()
    test = grouped_data
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ clean_text –∫ –∫–∞–∂–¥–æ–π —è—á–µ–π–∫–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º tqdm
    tqdm.pandas(miniters=100)
    test['otziv'] = test['otziv'].progress_apply(clean_text)
    # –°—Ç–µ–º–º–∏–Ω–≥
    stemmer = SnowballStemmer("russian")

    russian_stopwords = stopwords.words("russian")
    russian_stopwords.extend(['‚Ä¶', '¬´', '¬ª', '...', '—Ç.–¥.', '—Ç', '–¥'])

    stemmed_texts_list = []
    for text in tqdm(test['otziv']):
        tokens = word_tokenize(text)
        stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in russian_stopwords]
        text = " ".join(stemmed_tokens)
        stemmed_texts_list.append(text)

    test['text_stem'] = stemmed_texts_list

    print(test.head())

    return test

